### Keywords
- Configuration management
- Defect management
- Defect report
- Entry criteria
- Exit criteria
- Product risk
- Project risk
- Risk
- Risk level
- Risk-based testing
- Test approach
- Test control
- Test estimation
- Test manager
- Test monitoring
- Test plan
- Test planning
- Test progress report
- Test strategy
- Test summary report
- Tester

---

### Learning Objectives for Test Management

#### 5.1 Test Organization
- **FL-5.1.1 (K2)**: Explain the benefits and drawbacks of independent testing
- **FL-5.1.2 (K1)**: Identify the tasks of a test manager and tester

#### 5.2 Test Planning and Estimation
- **FL-5.2.1 (K2)**: Summarize the purpose and content of a test plan
- **FL-5.2.2 (K2)**: Differentiate between various test strategies
- **FL-5.2.3 (K2)**: Give examples of potential entry and exit criteria
- **FL-5.2.4 (K3)**: Apply knowledge of prioritization, and technical and logical dependencies, to schedule test execution for a given set of test cases
- **FL-5.2.5 (K1)**: Identify factors that influence the effort related to testing
- **FL-5.2.6 (K2)**: Explain the difference between two estimation techniques: the metrics-based technique and the expert-based technique

#### 5.3 Test Monitoring and Control
- **FL-5.3.1 (K1)**: Recall metrics used for testing
- **FL-5.3.2 (K2)**: Summarize the purposes, contents, and audiences for test reports

#### 5.4 Configuration Management
- **FL-5.4.1 (K2)**: Summarize how configuration management supports testing

#### 5.5 Risks and Testing
- **FL-5.5.1 (K1)**: Define risk level by using likelihood and impact
- **FL-5.5.2 (K2)**: Distinguish between project and product risks
- **FL-5.5.3 (K2)**: Describe, by using examples, how product risk analysis may influence the thoroughness and scope of testing

#### 5.6 Defect Management
- **FL-5.6.1 (K3)**: Write a defect report, covering a defect found during testing

### 5.1.1 Independent Testing

Testing tasks may be done by people in a specific testing role or by people in other roles (e.g., customers). A certain degree of **independence** helps testers find defects due to differences in cognitive biases (see section 1.5). However, **independence** is not a substitute for **familiarity** with the system, and developers can find many defects in their own code.

Degrees of independence in testing, from **low** to **high**, include:

- No independent testers; only developers testing their own code.
- Independent developers or testers within the development teams (e.g., developers testing each other's code).
- Independent test team or group within the organization, reporting to project management or executive management.
- Independent testers from the business organization, user community, or specialized test areas (e.g., usability, security, performance).
- Independent testers external to the organization, either on-site (in-house) or off-site (outsourced).

Having **multiple test levels** is ideal for most projects, with some levels managed by independent testers. Developers should still participate in testing, especially in the lower levels.

The implementation of independent testing varies based on the **software development lifecycle model**. For example, in **Agile** development, testers are part of the development team, and product owners may perform **acceptance testing** to validate user stories at the end of each iteration.

#### Potential Benefits of Test Independence:
- Independent testers are likely to recognize different kinds of failures due to their diverse perspectives and technical biases.
- They can verify or challenge assumptions made during specification and implementation.
- Independent testers can provide objective feedback without internal pressures.

#### Potential Drawbacks of Test Independence:
- Testers may become isolated from the development team, leading to lack of collaboration or adversarial relationships.
- Developers may lose a sense of ownership over the quality.
- Independent testers may be seen as bottlenecks.
- They may lack critical information about the test object.

Many organizations successfully implement test independence while mitigating the drawbacks.

---

### 5.1.2 Tasks of a Test Manager and Tester

This section covers the roles of **test managers** and **testers**. The responsibilities depend on the project, product context, individual skills, and the organization.

#### Test Manager Tasks:
The **test manager** has overall responsibility for the **test process** and leadership of test activities. This role might be performed by a **professional test manager** or by other roles such as project managers or quality assurance managers. In large organizations, multiple test teams may report to a test manager or coordinator.

Typical tasks of a test manager include:

- Develop or review a **test policy** and **test strategy**.
- Plan testing activities, considering the context and test objectives. This includes selecting approaches, estimating effort, and planning defect management.
- Write and update the **test plan(s)**.
- Coordinate test plans with project managers and product owners.
- Share testing perspectives with other project activities (e.g., integration planning).
- Oversee test execution, monitor progress, and check the status of **exit criteria**.
- Prepare and deliver **test progress reports** and **test summary reports**.
- Adapt planning based on test results and take action as necessary for **test control**.
- Set up defect management and support configuration management of testware.
- Introduce metrics to measure test progress and quality.
- Support tool selection and implementation for the test process.
- Decide on the implementation of the test environment.
- Promote the test team and profession within the organization.
- Develop the skills of testers through training and performance evaluations.

The **test manager** role varies depending on the **software development lifecycle**. In **Agile**, some of the test manager tasks are handled by the **Agile team**, especially those related to day-to-day testing. Tasks that span multiple teams or the organization may be handled by **test coaches**.

#### Tester Tasks:
Testers typically perform tasks such as:

- Review and contribute to **test plans**.
- Analyze and assess requirements, user stories, and acceptance criteria for **testability**.
- Identify and document **test conditions** and capture traceability between test cases and conditions.
- Design, set up, and verify the **test environment**.
- Implement test cases and procedures.
- Prepare and acquire **test data**.
- Create detailed **test execution schedules**.
- Execute tests, evaluate results, and document deviations.
- Use tools to facilitate the test process and automate tests when needed.
- Evaluate **non-functional characteristics** such as performance, usability, security, and compatibility.
- Review tests developed by others.

Depending on the **risk** and **development lifecycle**, different people may take the role of a tester at various levels. For example:

- **Component testing** and **component integration testing** are often performed by developers.
- **Acceptance testing** may be done by business analysts, subject matter experts, and users.
- **System testing** and **system integration testing** may be done by independent test teams.
- **Operational acceptance testing** is often performed by operations and system administration staff.

### 5.2 Test Planning and Estimation

#### 5.2.1 Purpose and Content of a Test Plan

A **test plan** outlines test activities for development and maintenance projects. It is influenced by factors such as:

- Test policy and strategy of the organization
- Development lifecycles and methods
- Scope of testing, objectives, risks, and constraints
- Criticality, testability, and resource availability

As the project progresses, more details can be included in the test plan, making it a continuous activity throughout the product's lifecycle, including the maintenance phase. Test planning should adapt to changing risks, adjusting based on feedback from test activities. A **master test plan** and separate plans for different **test levels** (e.g., system testing, acceptance testing) or **test types** (e.g., usability testing, performance testing) may be created.

Test planning activities may include the following:

- Determining scope, objectives, and risks of testing
- Defining the overall testing approach
- Integrating testing into the software lifecycle activities
- Deciding what to test, required resources, and how to execute test activities
- Scheduling test activities (analysis, design, execution)
- Selecting metrics for **test monitoring** and **control**
- Budgeting for test activities
- Determining the level of detail and structure for test documentation

The **content of test plans** can extend beyond these topics. Sample structures can be found in **ISO/IEC/IEEE 29119-3**.

---

#### 5.2.2 Test Strategy and Test Approach

A **test strategy** provides a generalized description of the test process at the product or organizational level. Common types include:

- **Analytical:** Based on analysis of factors like risk or requirements. Risk-based testing is an example.
- **Model-Based:** Tests are designed based on models like business processes, state models, or reliability.
- **Methodical:** Systematic use of predefined test conditions, such as common failure types or company standards.
- **Process-Compliant:** Tests follow external standards or rules, like industry-specific standards.
- **Directed (Consultative):** Guided by experts or stakeholders outside the test team.
- **Regression-Averse:** Focused on avoiding regressions, with extensive automation and reuse of testware.
- **Reactive:** Tests are designed reactively during execution, often using exploratory testing techniques.

An appropriate strategy often combines multiple approaches. For example, **risk-based testing** (analytical) may complement **exploratory testing** (reactive) for more effective testing.

The **test strategy** describes the overall process, while the **test approach** tailors it for a specific project or release. The approach determines test techniques, levels, types, and defines **entry** and **exit criteria** (or definition of ready/done). It is influenced by the projectâ€™s complexity, product type, and risk analysis. Factors like **safety**, available resources, system type, test objectives, and regulations may affect the choice of approach.

---

#### 5.2.3 Entry Criteria and Exit Criteria (Definition of Ready and Definition of Done)

To maintain control over software quality, it is important to define when test activities should start (**entry criteria**) and when they are considered complete (**exit criteria**).

**Entry criteria** (or definition of ready in Agile) ensure test activities are initiated under optimal conditions. If not met, testing may become difficult, costly, or risky.

**Exit criteria** (or definition of done in Agile) define when testing at a certain level or type is complete.

Typical **entry criteria** include:

- Availability of testable requirements, user stories, or models
- Test items meeting exit criteria from prior test levels
- Availability of the test environment and tools
- Availability of test data and resources

Typical **exit criteria** include:

- Planned tests executed
- Required coverage (requirements, stories, risks, code) achieved
- Number of unresolved defects within agreed limits
- Remaining defects estimated to be low
- Quality characteristics (e.g., reliability, performance, security) are sufficient

Sometimes, even without fulfilling all exit criteria, test activities may be stopped due to budget, time constraints, or pressure to release the product. If stakeholders accept the associated risks, it can be acceptable to end testing under such circumstances.

---

#### 5.2.4 Test Execution Schedule

Once test cases and procedures are assembled into test suites, a **test execution schedule** is created. This schedule defines the order in which tests are run, considering factors like:

- **Prioritization**: Running the highest priority tests first
- **Dependencies**: Executing dependent lower-priority tests first if needed
- **Confirmation and regression tests**: Prioritized for quick feedback on changes

In some cases, multiple test execution sequences are possible, requiring trade-offs between execution efficiency and adherence to prioritization.

### 5.2.5 Factors Influencing the Test Effort

**Test effort estimation** involves predicting the amount of test-related work required to meet the objectives for a project, release, or iteration. The following factors can influence the test effort:

#### Product Characteristics
- **Product risks**: Higher risks usually require more testing.
- **Quality of the test basis**: Well-defined requirements or user stories reduce uncertainty in testing.
- **Product size**: Larger products require more tests.
- **Product complexity**: More complex domains demand greater test effort.
- **Quality characteristics**: Specific needs like security or reliability increase testing effort.
- **Level of test documentation detail**: More detailed documentation requires more effort.
- **Legal and regulatory compliance**: Testing for compliance with regulations increases effort.

#### Development Process Characteristics
- **Organizational stability and maturity**: Stable and mature organizations often have established processes that impact testing.
- **Development model**: Different models (e.g., Agile, Waterfall) influence the testing effort.
- **Test approach**: Analytical or risk-based approaches may require different levels of effort.
- **Tools used**: Test automation and tools may reduce or increase effort.
- **Test process**: Efficient processes reduce effort, while inefficient ones increase it.
- **Time pressure**: Shorter timeframes can lead to more condensed and effort-intensive testing.

#### People Characteristics
- **Skills and experience**: More experienced testers can work more efficiently, reducing the effort.
- **Team cohesion and leadership**: A well-coordinated team can be more effective in their testing effort.

#### Test Results
- **Number and severity of defects found**: More defects, especially severe ones, increase rework and retesting.
- **Amount of rework required**: The higher the amount of rework, the greater the test effort needed.

---

### 5.2.6 Test Estimation Techniques

There are two commonly used **test estimation techniques**:

#### Metrics-Based Technique
- **Definition**: Estimation based on metrics from previous similar projects or typical values.
- **Example in Agile**: **Burndown charts** track remaining effort, feeding into team velocity to estimate future work.
- **Example in Sequential Projects**: **Defect removal models** track defect volumes and removal times to estimate future projects.

#### Expert-Based Technique
- **Definition**: Estimation based on the experience of experts or task owners.
- **Example in Agile**: **Planning poker**, where team members estimate effort based on their experience.
- **Example in Sequential Projects**: **Wideband Delphi** technique, where a group of experts provides estimates based on experience.
