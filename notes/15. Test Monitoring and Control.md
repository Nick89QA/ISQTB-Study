### Test Monitoring and Control

The **purpose of test monitoring** is to gather information and provide feedback and visibility about test activities. Information to be monitored may be collected manually or automatically and should be used to assess test progress and to measure whether the test exit criteria, or the testing tasks associated with an Agile project's definition of done, are satisfied. This includes meeting the targets for coverage of product risks, requirements, or acceptance criteria.

**Test control** describes any guiding or corrective actions taken as a result of information and metrics gathered and (possibly) reported. Actions may cover any test activity and may affect any other software lifecycle activity.

### Examples of Test Control Actions
- **Re-prioritizing tests**: When a risk is identified, such as software being delivered late, test priorities may be adjusted.
- **Changing the test schedule**: This may occur due to the availability or unavailability of a test environment or other resources.
- **Re-evaluating test items**: Assessing whether a test item meets entry or exit criteria after rework.

---

### 5.3.1 Metrics Used in Testing

Metrics are collected during and at the end of test activities to assess:
- **Progress**: Against planned schedule and budget.
- **Test object quality**: Current status and quality level.
- **Test approach adequacy**: How suitable the testing approach is.
- **Test activity effectiveness**: The effectiveness in meeting test objectives.

Common test metrics include:
- Percentage of planned work done in test case preparation or implementation.
- Percentage of planned work done in test environment preparation.
- Test case execution status: Test cases run, passed, failed, or not run.
- Defect information: Defect density, found/fixed defects, failure rate, and confirmation test results.
- Test coverage: Coverage of requirements, user stories, risks, or code.
- Resource allocation, task completion, and effort.
- Testing costs: Including the cost of finding the next defect or running the next test.

---

### 5.3.2 Purposes, Contents, and Audiences for Test Reports

**Purpose**: To summarize and communicate test activity information during and at the end of testing (e.g., a test level).
- **Test progress report**: Summarizes information during a test activity.
- **Test summary report**: Summarizes information at the end of a test activity.

**Test progress report** contents:
- Status of test activities and progress against the plan.
- Factors impeding progress.
- Testing planned for the next period.
- Quality of the test object.

**Test summary report** contents:
- Summary of testing performed.
- Events during the test period.
- Deviations from the plan (schedule, effort, or duration).
- Status of testing and product quality related to exit criteria.
- Metrics for defects, test cases, test coverage, activity progress, and resource consumption.
- Residual risks.
- Reusable test work products.

Test reports should be tailored to the project and audience:
- **Technical audience**: Focus on defect details and trends.
- **Executive summary**: High-level summaries of defects, budget, schedule, and test status.

---

### 5.4 Configuration Management

**Purpose**: To maintain the integrity of components, systems, testware, and their relationships throughout the lifecycle.

To support testing, configuration management ensures:
- All test items are uniquely identified, version-controlled, tracked, and related.
- All testware is uniquely identified, version-controlled, and traceable to test items.
- Unambiguous reference to documents and software items in test documentation.

---

### 5.5 Risks and Testing

#### 5.5.1 Definition of Risk
Risk is the possibility of a future event with negative consequences, measured by its likelihood and impact.

#### 5.5.2 Product and Project Risks
**Product risks**: Risks that a work product may fail to meet user or stakeholder needs, especially regarding quality characteristics (e.g., reliability, performance).

Examples of product risks:
- Software may not perform intended functions according to specifications or user needs.
- System architecture may not support non-functional requirements.
- A computation may be incorrect in specific scenarios.
- Response times may be inadequate in high-performance systems.
- User experience feedback may fall short of expectations.
